'use client'

import { useEffect, useRef, useState } from 'react'
import * as faceapi from 'face-api.js'

interface FaceBlurProcessorProps {
  imageSrc: string
  onProcessingChange: (processing: boolean) => void
}

interface FaceRegion {
  x: number
  y: number
  width: number
  height: number
}

export default function FaceBlurProcessor({ imageSrc, onProcessingChange }: FaceBlurProcessorProps) {
  const canvasRef = useRef<HTMLCanvasElement>(null)
  const [processedImage, setProcessedImage] = useState<string | null>(null)
  const [faces, setFaces] = useState<FaceRegion[]>([])
  const [isDetecting, setIsDetecting] = useState(false)
  const [modelsLoaded, setModelsLoaded] = useState(false)

  // face-api.js ëª¨ë¸ ë¡œë“œ (ê°„ì†Œí™”)
  useEffect(() => {
    const loadModels = async () => {
      try {
        // ì–¼êµ´ ê°ì§€ë§Œ í•„ìš”í•œ ê²½ìš° tinyFaceDetectorë§Œ ë¡œë“œ
        await faceapi.nets.tinyFaceDetector.loadFromUri('/models')
        setModelsLoaded(true)
        console.log('ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!')
      } catch (error) {
        console.error('ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨:', error)
        setModelsLoaded(true) // ì—ëŸ¬ê°€ ìˆì–´ë„ ê³„ì† ì§„í–‰
      }
    }

    loadModels()
  }, [])

  // ì–¼êµ´ ì˜ì—­ì— blur ì ìš©
  const applyBlurToFaces = (canvas: HTMLCanvasElement, faces: FaceRegion[]) => {
    const ctx = canvas.getContext('2d')
    if (!ctx) return

    faces.forEach(face => {
      // ì–¼êµ´ ì˜ì—­ì„ ìµœì í™”ëœ í¬ê¸°ë¡œ í™•ì¥
      const expansion = Math.min(face.width * 0.15, 25)
      const expandedFace = {
        x: Math.max(0, face.x - expansion),
        y: Math.max(0, face.y - expansion),
        width: Math.min(face.width + expansion * 2, canvas.width - face.x),
        height: Math.min(face.height + expansion * 2, canvas.height - face.y)
      }

      // ì–¼êµ´ ì˜ì—­ì˜ ì´ë¯¸ì§€ ë°ì´í„° ì¶”ì¶œ
      const imageData = ctx.getImageData(
        expandedFace.x, 
        expandedFace.y, 
        expandedFace.width, 
        expandedFace.height
      )

      // ìµœì í™”ëœ blur ì ìš©
      const blurredData = applyBoxBlur(imageData, 20)
      
      // blurëœ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ìº”ë²„ìŠ¤ì— ê·¸ë¦¬ê¸°
      ctx.putImageData(blurredData, expandedFace.x, expandedFace.y)
    })
  }

  // ë°•ìŠ¤ blur ì•Œê³ ë¦¬ì¦˜
  const applyBoxBlur = (imageData: ImageData, radius: number): ImageData => {
    const { data, width, height } = imageData
    const result = new Uint8ClampedArray(data.length)
    
    for (let y = 0; y < height; y++) {
      for (let x = 0; x < width; x++) {
        let r = 0, g = 0, b = 0, a = 0, count = 0
        
        // ì£¼ë³€ í”½ì…€ë“¤ì˜ í‰ê·  ê³„ì‚°
        for (let dy = -radius; dy <= radius; dy++) {
          for (let dx = -radius; dx <= radius; dx++) {
            const nx = x + dx
            const ny = y + dy
            
            if (nx >= 0 && nx < width && ny >= 0 && ny < height) {
              const idx = (ny * width + nx) * 4
              r += data[idx]
              g += data[idx + 1]
              b += data[idx + 2]
              a += data[idx + 3]
              count++
            }
          }
        }
        
        const idx = (y * width + x) * 4
        result[idx] = r / count
        result[idx + 1] = g / count
        result[idx + 2] = b / count
        result[idx + 3] = a / count
      }
    }
    
    return new ImageData(result, width, height)
  }

  useEffect(() => {
    if (!imageSrc || !canvasRef.current || !modelsLoaded) return

    const processImage = async () => {
      setIsDetecting(true)
      onProcessingChange(true)

      const canvas = canvasRef.current!
      const ctx = canvas.getContext('2d')!
      
      const img = new Image()
      img.onload = async () => {
        canvas.width = img.width
        canvas.height = img.height
        ctx.drawImage(img, 0, 0)
        
        // face-api.jsë¡œ ì–¼êµ´ ì¸ì‹ (ì—ëŸ¬ ì²˜ë¦¬ ì¶”ê°€)
        let detectedFaces: FaceRegion[] = []
        try {
          const detections = await faceapi.detectAllFaces(img, new faceapi.TinyFaceDetectorOptions())
          detectedFaces = detections.map(detection => ({
            x: detection.box.x,
            y: detection.box.y,
            width: detection.box.width,
            height: detection.box.height
          }))
          console.log('ì–¼êµ´ ê°ì§€:', detectedFaces.length, 'ê°œ')
        } catch (error) {
          console.error('ì–¼êµ´ ì¸ì‹ ì‹¤íŒ¨:', error)
          detectedFaces = [] // ë¹ˆ ë°°ì—´ë¡œ ì„¤ì •
        }

        setFaces(detectedFaces)
        
        applyBlurToFaces(canvas, detectedFaces)
        
        setProcessedImage(canvas.toDataURL('image/jpeg', 0.9))
        setIsDetecting(false)
        onProcessingChange(false)
      }
      
      img.src = imageSrc
    }

    processImage()
  }, [imageSrc, onProcessingChange, modelsLoaded])

  const downloadImage = () => {
    if (!processedImage) return
    
    const link = document.createElement('a')
    link.download = 'blurred-face.jpg'
    link.href = processedImage
    link.click()
  }

  return (
    <div className="space-y-4">
      {!modelsLoaded && (
        <div className="bg-yellow-50 border border-yellow-200 rounded p-4">
          <p className="text-yellow-800">ğŸ¤– AI ëª¨ë¸ ë¡œë”© ì¤‘... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”!</p>
        </div>
      )}
      
      <div className="grid grid-cols-1 md:grid-cols-2 gap-4">
        {/* ì›ë³¸ ì´ë¯¸ì§€ */}
        <div>
          <h3 className="text-lg font-semibold mb-2">ì›ë³¸ ì´ë¯¸ì§€</h3>
          <img 
            src={imageSrc} 
            alt="ì›ë³¸" 
            className="w-full h-auto border rounded"
          />
        </div>
        
        {/* ì²˜ë¦¬ëœ ì´ë¯¸ì§€ */}
        <div>
          <h3 className="text-lg font-semibold mb-2">
            ì²˜ë¦¬ëœ ì´ë¯¸ì§€ {isDetecting && '(ì²˜ë¦¬ ì¤‘...)'}
          </h3>
          {processedImage ? (
            <div className="relative">
              <img 
                src={processedImage} 
                alt="ì²˜ë¦¬ë¨" 
                className="w-full h-auto border rounded"
              />
              <div className="absolute top-2 right-2 bg-green-500 text-white px-2 py-1 rounded text-sm">
                {faces.length}ê°œ ì–¼êµ´ ê°ì§€ë¨
              </div>
            </div>
          ) : (
            <div className="w-full h-64 bg-gray-100 border rounded flex items-center justify-center">
              {isDetecting ? 'ì–¼êµ´ ì¸ì‹ ì¤‘...' : 'ì²˜ë¦¬ ëŒ€ê¸° ì¤‘'}
            </div>
          )}
        </div>
      </div>

      {/* ì–¼êµ´ ê°ì§€ ê²°ê³¼ ìš”ì•½ */}
      {faces.length > 0 && (
        <div className="bg-green-50 border border-green-200 rounded-lg p-4">
          <div className="flex items-center gap-2">
            <div className="w-2 h-2 bg-green-500 rounded-full"></div>
            <span className="font-semibold text-green-800">
              {faces.length}ê°œì˜ ì–¼êµ´ì´ ê°ì§€ë˜ì–´ ë¸”ëŸ¬ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤
            </span>
          </div>
          <p className="text-sm text-green-700 mt-1">
            ê°œì¸ì •ë³´ê°€ ë³´í˜¸ëœ ì´ë¯¸ì§€ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì—¬ ë¯¼ì› ì‘ì„±ì— ì‚¬ìš©í•˜ì„¸ìš”.
          </p>
        </div>
      )}

      {/* ì•¡ì…˜ ë²„íŠ¼ë“¤ */}
      {processedImage && (
        <div className="space-y-3">
          <button
            onClick={downloadImage}
            className="w-full h-12 rounded-2xl shadow-lg bg-card text-foreground border-2 border-border font-medium hover:bg-card/80 hover:border-primary/50 transition-all duration-200 transform hover:scale-[1.02] active:scale-[0.98]"
          >
            ğŸ“¥ ì²˜ë¦¬ëœ ì´ë¯¸ì§€ ë‹¤ìš´ë¡œë“œ
          </button>
        </div>
      )}

      {/* ìˆ¨ê²¨ì§„ ìº”ë²„ìŠ¤ */}
      <canvas ref={canvasRef} className="hidden" />
    </div>
  )
}
